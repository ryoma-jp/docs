# AI関連年表

* IC: Image Classifiacation
* OD: Object Detection
* SS: Semantic Segmentation
* NLP: Natural Language Processing
* TS: Time Series
* Gen: Generation

|年|技術|IC|OD|SS|NLP|TS|Gen|関連情報|
|---|---|---|---|---|---|---|---|---|
|1958|パーセプトロン| :heavy_check_mark: |||| :heavy_check_mark: |||
||ロジスティック回帰| :heavy_check_mark: |||| :heavy_check_mark: |||
|1963|サポートベクターマシン(SVM)| :heavy_check_mark: |||| :heavy_check_mark: |||
|1980|「強いAI・弱いAI」の概念の登場||||||||
|1986|誤差逆伝播||||||| [Backpropagation Through Time](http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/pdfs/Werbos.backprop.pdf) |
|1991|Python||||||||
|1995|LSTM|||| :heavy_check_mark: | :heavy_check_mark: |||
|1998|LeNet| :heavy_check_mark: |||||||
|2006|積層オートエンコーダ|||||| :heavy_check_mark: ||
|2012|AlexNet| :heavy_check_mark: |||||||
|2014|VGG| :heavy_check_mark: |||||||
||GoogLeNet| :heavy_check_mark: |||||||
||GAN|||||| :heavy_check_mark: ||
|2015|ResNet| :heavy_check_mark: |||||||
|2016|SqueezeNet| :heavy_check_mark: |||||| [SQUEEZENET](https://arxiv.org/pdf/1602.07360.pdf) |
|2017|MobileNet(v1)| :heavy_check_mark: |||||| [MobileNets](https://arxiv.org/pdf/1704.04861.pdf) |
|2018|MobileNet(v2)| :heavy_check_mark: |||||| [MobileNetV2](https://arxiv.org/pdf/1801.04381v3.pdf) |
||BERT|||| :heavy_check_mark: ||||
|2019|MobileNet(v3)| :heavy_check_mark: |||||| [Searching for MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf) |


# 参考

* [【資格対策にも！】ダートマス会議からシンギュラリティまでAIの歴史を年表でおさらい](https://ainow.ai/2020/03/25/193030/)
* [深層学習／GoogLeNet, ResNet](https://qiita.com/jun40vn/items/5ac97a6f1d8f82a49194)
* [MobileNet(v1,v2,v3)を簡単に解説してみた](https://qiita.com/omiita/items/77dadd5a7b16a104df83)
* [[Survey]SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size](https://qiita.com/supersaiakujin/items/ece1e20ca4073e77bed7)
* [今度こそわかるぞRNN, LSTM編](https://qiita.com/kazukiii/items/df809d6cd5d7d1f57be3)
* [回帰型ニューラルネットワーク](https://ja.wikipedia.org/wiki/%E5%9B%9E%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF)
* [RNNからTransformerまでの歴史を辿る ～DNNを使ったNLPを浅く広く勉強～](https://aru47.hatenablog.com/entry/2020/08/18/175711)
* [Recurrent Neural Networkとは何か、他のニューラルネットワークと何が違うのか](https://www.atmarkit.co.jp/ait/articles/1608/26/news011.html#03)
* [【論文シリーズ】新しいRNNの学習方法（Hessian FreeとRBM利用）](https://qiita.com/To_Murakami/items/d32ed128640ebd18b977)
* [Recurrent Neural Networks (1) – RNN への序説 （翻訳/要約）](https://tensorflow.classcat.com/2016/03/17/introduction-to-rnn/)
* [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
* [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)
* [ディープラーニングは、時系列予測でも最強なのか？　～RNNと従来手法との対比から見える使いどころ～](https://www.sas.com/content/dam/SAS/documents/marketing-whitepapers-ebooks/sas-whitepapers/ja/viya-recurrent-neural-network.pdf)
* [わかるLSTM ～ 最近の動向と共に](https://qiita.com/t_Signull/items/21b82be280b46f467d1b)
* [複数入力を用いたRecurrent Neural Networkに基づく時系列予測](https://www.ieice.org/publications/conference-FIT-DVDs/FIT2019/data/pdf/CF-001.pdf)
