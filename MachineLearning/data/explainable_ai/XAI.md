# Explainable AI

|Method|Description(ChatGPT)|
|:--|:--|
|Transformer-based Explanations|Transformer-based Explanationsは、Transformerモデルを使用して、モデルの意思決定を説明する技術です。Transformer-based Explanationsは、Transformer Attention Weightと呼ばれるものを使用して、モデルがどの入力に注意を払っているかを説明します。|
|Contrastive Explanations Method|Contrastive Explanations Methodは、二つの入力の違いを説明することで、モデルの意思決定を説明する技術です。これにより、モデルがなぜそのような判断を下したのかを説明することができます。|
|Counterfactual Explanations|Counterfactual Explanationsは、モデルの意思決定に影響を与えることなく、その意思決定を変更することができるような入力を生成する技術です。これにより、モデルの意思決定を説明し、改善することができます。
|Integrated Gradients|Integrated Gradientsは、モデルの意思決定に影響を与える入力の重要度を計算する技術です。これにより、モデルの意思決定を説明することができます。|
|Local Interpretable Model-Agnostic Explanations|LIMEは、ブラックボックスモデルの説明を可能にする技術です。LIMEは、モデルがどのような入力に反応するかを説明するために、類似した入力を生成し、それらの入力とモデルの応答の関係を分析します。|
|SHapley Additive exPlanations|SHAPは、個々の特徴がモデルの予測にどのように影響を与えるかを説明するための技術です。SHAPは、ゲーム理論の考え方を使用して、特徴がモデルの予測に与える影響を解析します。|
|Grad-CAM|Grad-CAMは、畳み込みニューラルネットワークがどの部分に注意を払っているかを可視化するための技術です。Grad-CAMは、特徴マップの勾配を使用して、畳み込みニューラルネットワークがどの部分に注意を払っているかを計算します。|
|Attention-based Explanations|Attention-based Explanationsは、TransformerやLSTMなどのモデルで使用される注意機構を使用して、モデルがどの入力に注意を払っているかを説明する技術です。これにより、モデルがどのような情報を使用して意思決定を行ったのかを説明することができます。|
|Decision Trees-based Explanations|決定木を使用して、モデルの意思決定を説明する技術です。決定木は、入力の特徴に基づいて、モデルがどのように意思決定を行ったのかを説明することができます。|
|Layer-wise Relevance Propagation|LRPは、畳み込みニューラルネットワークの出力に対して、各層の入力がどの程度重要であるかを計算する技術です。これにより、ニューラルネットワークがどの部分に注目しているかを説明することができます。|
|Prototype and Feature Importance|PFIは、モデルの意思決定に対して重要な特徴を決定するために、プロトタイプの作成を組み合わせた手法です。PFIは、LIMEやSHAPと同様に、ブラックボックスモデルの説明を可能にします。|
|Anchor-based Explanations|Anchor-based Explanationsは、特定の条件が与えられた場合にモデルの予測がどのように変化するかを説明する技術です。これにより、モデルの予測がどのような条件に依存しているかを理解することができます。|
|Concept Activation Vectors|CAVsは、ニューラルネットワークの出力に対する特徴の重要度を評価するために使用される技術です。CAVsは、特定の特徴がどのように出力に貢献しているかを理解することを目的としています。|
|Partial Dependence Plots|PDPsは、ある特徴が出力に与える影響を可視化するために使用される技術です。PDPは、単一の特徴を選択し、その特徴が変化した場合のモデルの予測をプロットします。|
|Individual Conditional Expectation plots|ICEプロットは、PDPプロットと似ていますが、個々のサンプルに対して異なる線を描画することで、個々のサンプルにおける特徴の影響を詳細に解析します。|
|Anchors|アンカーは、個々の予測に対して特徴の説明を提供するために使用される技術です。アンカーは、条件を定義して、その条件が満たされる場合に予測がどのように変化するかを説明します。|
|Model Agnostic Meta-Learning|MAMLは、モデルの内部構造に関係なく、小さな量のデータで高い精度を達成するための技術です。MAMLは、新しいタスクに適用する前に、事前トレーニングされたモデルを調整するためのメタ学習を使用します。|
|Generative Counterfactual Explanations|GCEは、個々の予測に対して、カウンターファクトの生成を使用して説明を提供する技術です。GCEは、与えられた入力に対して、モデルが出力したクラス以外のクラスを予測するために必要な最小限の変更を生成します。|
|Global Surrogate Models|グローバルサロゲートモデルは、元のモデルと同じ予測を出力する代替モデルを作成する技術です。グローバルサロゲートモデルは、ブラックボックスモデルの予測を理解するために使用されます。|
|Counterfactual Explanations for Machine Learning|CEMは、カウンターファクトの生成を使用して、個々の予測に対する説明を提供する技術です。CEMは、モデルの予測を変更するために必要な最小限の変更を生成します。|
|Shapley Values|シャプリー値は、ある特徴が予測にどのような貢献をするかを測定するための技術です。シャプリー値は、特徴の寄与度を数学的に厳密に解釈できるため、公平な特徴選択や特徴の重要度のランキングに使用されます。|
|Tree-based Explanation Methods|決定木やランダムフォレストなどのツリーベースのモデルでは、ツリーの分岐や葉の条件を解釈することができます。これにより、特徴の重要性を把握し、モデルの意思決定を理解することができます。|
|Influence Functions|影響力関数は、個々のデータポイントが予測に与える影響を計算するための技術です。影響力関数は、モデルが誤った予測をした場合にどのデータポイントが原因であるかを特定するために使用されます。|
|Gradient-weighted Class Activation Mapping (GradCAM++)|GradCAM++は、モデルが予測を行う際に重要な領域を可視化するための技術です。GradCAM++は、畳み込みニューラルネットワークの中間層の重み付け特徴マップを計算します。|
|Layer-wise Relevance Propagation with Context|LRPwCは、ニューラルネットワークの各レイヤーの重要性を推定するための技術です。LRPwCは、予測に重要な要素を強調するために、他のExplainable AI技術と組み合わせて使用されます。|
|RuleFit|RuleFitは、線形モデルと決定木を組み合わせて使用する技術です。RuleFitは、モデルの予測を説明するために、特徴の相互作用を考慮したルールを生成します。|
|Local Rule-based Explanations|LoREは、個々の予測に対してローカルな説明を提供するための技術です。LoREは、モデルの局所的な決定プロセスを理解するために使用されます。|
|DeepLIFT|ニューラルネットワークに対して入力の重要性を計算する方法です。|
|Testing with Concept Activation Vectors|ニューラルネットワークが特定の概念（例：黒い犬）を理解しているかどうかをテストする方法です。|
|SHAPLEY-Sampling|Shapley Valuesを計算するために、膨大な数の組み合わせを網羅的に計算する代わりに、ランダムサンプリングに基づいて計算を高速化します。|
|LRP-γ|レイヤーごとの重要度スコアに加えて、背景知識やコンテキストを含めた重要度スコアを提供する方法です。|
|SHAP Interaction Values|SHAP Valuesの一般化で、相互作用項（2つ以上の特徴の組み合わせに基づく影響）を考慮することができます。|
|Randomized Input Sampling for Explanation of Black-box Models(RISE)|CNNの特徴マップのランダムなマスクを生成して、モデルの出力に与える影響を観察することにより、特徴マップの重要度を計算する方法です。|
|TREPAN|決定木の学習アルゴリズムを使用して、ルールベースの説明を生成する方法です。|
|Protodash|複数のインスタンスから代表的なインスタンスを見つけ出し、その代表的なインスタンスに対する重要度を計算する方法です。|
|Anchor-free explanation|ある入力インスタンスに対して、どのような変数が重要であるかを決定する方法です。Anchor-based explanationとは異なり、背景となるインスタンスの選択は不要です。|
|SHAP Tree Explainer|SHAP Tree Explainerは、決定木やランダムフォレストなどのツリーベースのモデルの解釈を可能にする手法です。これは、SHAP値を計算するために、決定木のルートから葉までのシンプルなトラバースを使用します。|
|GAM-based Explanations: Generalized Additive Models|目的変数と独立変数との間の非線形関係をモデル化することができます。GAMを用いた解釈手法では、個々の特徴量がどの程度モデルの予測に貢献しているかを示すパーシャル関数が生成されます。|
|Representative Feature Selection: Representative Feature Selection|特徴量選択に対する解釈可能性の問題に取り組むために提案された手法です。RFSは、データ内の複数の代表的なサンプルを選択し、そのサンプルに関連する重要な特徴量を選択することによって、特徴量選択の解釈可能性を向上させます。|
|Counterfactual Explanations via Kernel-Based Generative Models|カーネルベースの生成モデルを使用して、Counterfactual Explanationsを生成する手法です。モデルは、元のサンプルに最も近いカウンターファクチュアルなサンプルを生成することで、予測を解釈可能な形式で説明します。|
|Deep Counterfactual Regret Analysis|Counterfactual Explanationsに基づく強化学習に関する新しいフレームワークを提供します。これにより、モデルの不良な予測を回避することができます。この手法は、特定の状況での最適な行動を特定することで、不良な予測を回避することができます。|
|Contrastive Attribution Method (CAM)|予測結果に寄与するピクセルを可視化するための手法であり、予測クラスと最も似ていないクラスの特徴を比較することで、予測結果の根拠を理解することができる。|
|Layer-wise Relevance Propagation-Deep Taylor Decomposition (LRP-DTD)|ネットワーク内の各層における各ノードの重要度を解釈可能にするために使用されるExplainable AI技術で、各層の出力に基づいて入力に重要度を再分配することにより、重要な入力を特定することができます。|
|Layer-wise Relevance Propagation-Z (LRP-Z)|ニューラルネットワークの入力に関する重要性を推定するための解釈可能性手法であり、Layer-wise Relevance Propagation (LRP)の一種です。LRP-Zは、各層の出力に対して、入力に対する重要度の値を再帰的に伝播させることで、出力の寄与度を推定します。具体的には、各層での入力の寄与度を計算し、前の層に伝播することで、最終的に入力の重要度を求めます。LRP-Zは、ニューラルネットワークの各層の活性化関数に対して異なる重要度割り当て手法を使用することができます。|
|Layer-wise Relevance Propagation-ϵ (LRP-ϵ)|ニューラルネットワークの入力に対する出力の重要性を説明するために使用される技術の一つで、Layer-wise Relevance Propagation (LRP)の変種です。LRP-ϵは、与えられた入力と出力の間にエッジを付けたグラフを生成し、これに基づいてニューラルネットワーク内での情報のフローを解釈します。LRP-ϵは、畳み込みニューラルネットワークのような、より複雑なモデルでの解釈を可能にするために提案されました。|
|Attention-based Feature Extraction (AFE)|Attention mechanismを用いて、モデル内の重要な特徴量を抽出し、その特徴量を用いてモデルの予測を解釈する手法です。AFEは、入力データに対してAttentionを適用し、Attentionによって得られた重要な特徴量のみを取り出して、後続のタスクに利用することができます。この手法は、Transformerモデルによって広く使用され、特に自然言語処理の分野で良い結果を示しています。|
|Gradient-weighted Class Saliency Map (Grad-CAM++)|画像認識タスクにおいて、ニューラルネットワークの出力に対してどの部分が重要であるかを可視化するための手法の1つです。Grad-CAM++は、Grad-CAMにバイアス補正を加えた改良版であり、複数の畳み込み層を考慮し、より正確な可視化結果を得ることができます。Grad-CAM++は、モデルの予測結果に対して、どの特徴マップがより貢献しているかを明らかにすることができ、モデルの判断根拠を解釈するために利用されます。|
|Layer-wise Relevance Propagation-PatternNet (LRP-PN)|Layer-wise Relevance Propagation (LRP)の一種で、高次元特徴マップの重要性を可視化するために使用されます。LRP-PNは、神経回路を分析することにより、入力画像の分類に貢献している重要なパターンを抽出することができます。LRP-PNは、LRPの反転性を用いて、最終的な出力に影響を与える入力の特徴量を抽出し、重要性を可視化します。この方法は、画像分類や物体検出のようなコンピュータビジョンタスクに適用されています。|
|Deep SHAP (SHAP for deep learning models)|SHAP (SHapley Additive exPlanations)アルゴリズムを深層学習モデルに適用することで、個々の特徴量がモデルの予測にどの程度寄与しているかを解釈可能な形式で計算するための手法です。Deep SHAPは、モデル内の各特徴量の重要性を示すシャープな値を提供するため、モデルの解釈性を高めることができます。また、異なる特徴量の組み合わせに対する予測の変化を計算することで、モデルの予測の説明をより一層深めることができます。|
|SHAP DeepExplainer|Deep Learningモデルの予測における各特徴量の重要度を解釈可能な形式で計算するために使用されるSHAP（SHapley Additive exPlanations）の一種です。SHAP DeepExplainerは、ディープラーニングモデルの内部構造に基づいて、入力特徴量がモデル予測にどの程度貢献しているかを解釈可能な方法で計算することができます。SHAP DeepExplainerは、モデルの各層から抽出された重要な特徴量を利用することができるため、より正確な解釈を提供することができます。|
|Permutation Feature Importance|機械学習モデルの特徴量の重要度を評価するための方法で、ランダムに特徴量を並び替え、モデルの性能がどの程度変化するかを測定し、その特徴量の重要度を決定する手法です。ランダムな並び替えを行うことで、各特徴量がモデルの予測にどの程度影響を与えるかを評価し、モデルの特徴量の重要度を説明することができます。|
|Partial Dependence Plots with Interaction Effects|機械学習モデル内で複数の変数間の相互作用がある場合に、それらの相互作用を可視化するための手法です。従来のPartial Dependence Plotsでは、1つの変数が他の変数と相互作用している場合には適用できず、PDP-IEでは、変数間の相互作用を2次元プロットで表示することができます。これにより、より複雑なモデルを解釈し、より深い理解を得ることができます。|
|Partial Dependence Plots with Component Effects|特定の変数が予測に与える影響を可視化する方法の1つであり、変数ごとに1つのプロットを生成します。ただし、変数間の相互作用が考慮され、その影響を示します。これは、個々の変数の影響だけでなく、変数間の相互作用による複雑な効果を視覚的に分析することができるため、より包括的なモデルの解釈を可能にします。|
|Individual Conditional Expectation with Global Surrogate Model|個々のデータインスタンスに対して、変数の値を1つの変数に固定して残りの変数を変化させたときの目的関数の期待値をプロットすることによって、グローバルなモデルの解釈可能性を向上させる手法です。この手法は、グローバルなモデルの理解を深めるために、ローカルモデルのインスタンスごとの解釈可能性を補完することができます。|
|Anchors for regression models|回帰モデルに対してAnchorを適用する手法であり、モデルが特定の出力を行うために必要な入力特徴量の範囲を決定することができます。つまり、入力特徴量の特定の範囲が与えられた場合、モデルが与えられた目標変数の値を予測するかどうかを判断することができます。この手法は、線形回帰、ランダムフォレスト、勾配ブースティングなどの回帰アルゴリズムに適用できます。|
|TreeExplainer|SHAPの一部であり、木構造を含む機械学習モデルの予測の根本原因を説明するために使用されるアルゴリズムです。TreeExplainerは、決定木、ランダムフォレスト、グラディエントブースティング木、およびLightGBMのような木ベースのモデルに最適化されており、SHAP値を計算するための基礎となるアルゴリズムを提供します。|
|L2X|ディープニューラルネットワークにおいて、モデルがどの入力特徴量に基づいて予測を行っているかを解釈可能な形で説明する手法の1つです。L2Xは、入力特徴量を2つのグループに分け、一方のグループから予測値を計算する際にもう一方のグループの情報を使わないように、特徴量を重要度に基づいてマスキングすることで、モデルがどの特徴量に注目して予測を行っているかを把握することができます。|
|Integrated Hessians|統合勾配法に基づくモデルの解釈可能性手法の1つであり、入力特徴量の重要性を推定するために使用されます。勾配法に基づく手法は、モデルの微分可能性を利用して特徴量の重要性を決定するため、Integrated Hessiansは、より高次元の特徴量空間において勾配法の精度を向上させるため、勾配の二次微分（Hessian）を使用します。|
|Deep Taylor (DT)|ディープニューラルネットワーク内部の各入力変数の寄与度を評価するためのアプローチであり、Layer-wise Relevance Propagation (LRP)の一種です。DTでは、各ニューロンが寄与する重みを、そのニューロンが活性化するときの出力の増分を使用して計算します。これにより、DTはニューロンがどの程度重要であるかを計算し、入力変数の重要性を推定することができます。|
|Feature Ablation through Hierarchical Decomposition (FAHD)|モデルの予測に対して重要な特徴を特定するための方法であり、特徴の階層的な削除に基づいています。まず、特徴をグループ化し、各グループを削除した場合のモデルの精度を計算し、各グループの重要度を決定します。次に、重要度の高いグループから順に、グループ内の各特徴を削除することで、特徴の重要度をより詳細に決定します。このようにして、FAHDは、特徴の重要度をより詳細に解釈できるようにすることができます。|
|Hierarchical Explanations (HI-LIME)|複雑なデータを持つモデルに対して、分類結果の説明を提供するための手法であり、LIMEと同様に局所的にモデルを線形近似することに基づいています。しかし、LIMEでは局所的な説明しか提供できませんが、HI-LIMEはより階層的な説明を提供することができ、特徴の重要性をより詳細に解釈することができます。HI-LIMEは、LIMEに加えて、特徴の重要性を決定するための統計的な手法を使用しています。|
|Model-Agnostic Convolutional Explanations (MACER)|モデルアーキテクチャに依存しない畳み込みニューラルネットワークの局所的な説明を生成するための手法です。MACERは、Convolutional Neural Network (CNN)での畳み込み演算の結果に対する貢献度を決定するために、エキスパートシステムに基づく手法を採用しています。MACERは、CNNの最後の層から出力された特徴マップに基づいて、各入力ピクセルの重要度を評価することができます。|
|Spline-based Interpretation (SBI)|決定木やランダムフォレストなどのブラックボックスモデルに対して、個々の予測の影響を評価するために、Spline Interpolationを使用するモデルアグノスティックな手法です。SBIは、可解釈性が高く、特徴量の相互作用を分析することができます。|
|CEM-NAF|Contrastive Explanation Method with Neural Additive Factorsと呼ばれ、ロジスティック回帰分類器を用いて、予測に影響を与えた特徴量の組み合わせを見つけ出すモデルアクションの手法です。CEM-NAFは、特徴量の重要性を表す線形モデルとしての結果を出力するため、モデルの透明性を高めることができます。また、CEM-NAFは、画像、テキスト、テーブルデータなどの複数のデータタイプに適用することができます。|
|Randomized Dependence Coefficient (RDC)|2つのデータセットの相互依存性を測定するために使用される非線形相関尺度の1つで、カーネル相関係数を基に構築されています。この手法は、主成分分析や相互情報量などの他の相関係数手法に比べて、非線形性に強く、高次元データにも適用可能です。|
|Invariant Risk Minimization (IRM)|分布のシフトに対して強くロバストなモデルを学習する方法であり、ドメイン適応の問題に対処するために使用されます。IRMは、環境の違いに関係なく、複数の環境で同じようにうまく機能するモデルを学習することを目的としています。IRMは、埋め込み空間における各環境の影響を取り除くことで、環境間の共通点を特定し、それらに焦点を当てることによって実現されます。|
|Counterfactual Explanations with Multi-Agent Adversarial Inverse Reinforcement Learning (MARL-CEX)|複数のエージェントが関与する環境において、人工知能の意思決定プロセスを説明する手法であり、カウンターファクチュアル（対事実）な解釈を生成することができます。MARL-CEXでは、エージェントが複数存在する場合でも、各エージェントの行動に基づいて生成される説明が一貫性を持つように、逆強化学習と敵対的生成ネットワーク（GAN）を用いた手法が採用されています。|
|Probabilistic Integrated Gradients (PIG)|統計的な意味を持つ統合グラデーションを計算する方法であり、分類器が不確実性を持つ場合に重要な特徴量を識別することができます。PIGは、統合グラデーションを計算するためにMCドロップアウトを使用することで、モデルの不確実性をモデル化します。|
|Partial Dependence-based Variable Importance (PDVI)|Partial Dependence Plot（PDP）を用いて特徴量の重要度を評価する手法で、予測値の変化に対する各特徴量の影響度を把握することができます。PDPはある特徴量を1つ変化させたときに、他の特徴量を平均した場合のモデルの予測値の変化を可視化する手法です。PDVIはこのPDPを用いて、特徴量を個別に変化させた場合の予測値の変化の大きさを評価し、その特徴量の重要度を算出します。|
|Approximate Shapley Values for Large-Scale Interactions (ASV-L2X)|大規模な相互作用を持つ特徴量のSHAP値を効率的に近似する手法で、機能を重要度に分解するために使用されます。この手法は、L2Xと呼ばれるディープラーニングのためのフレームワークに基づいています。ASV-L2Xでは、重要な特徴量の選択と、その特徴量に対するSHAP値の計算を同時に行うことができます。そのため、計算時間を短縮しながら、大規模な特徴量に対しても高い精度を実現することができます。|
|Norm-based Importance and Pruning (NIP)|ニューラルネットワークの重みの重要性を決定し、重要でない重みを削除するために使用される手法です。具体的には、各層の重みをL1ノルムで正規化した後、重要な重みを選択し、重要でない重みを削除します。これにより、モデルの複雑性が低減され、過学習を防止することができます。|
|Unmasking|ディープラーニングモデル内部の潜在的なバイアスを特定するための手法で、モデルの出力を反映しない入力の一部をランダムにマスクすることで、どの特徴がモデルの予測に寄与しているかを分析します。Unmaskingにより、モデルがどの特徴に注意を払って予測を行っているかを理解することができ、モデルの改善や信頼性の向上に役立ちます。|
|Analysis by Synthesis (ABS)|機械学習モデルの予測結果に基づいて、入力データを再構築することによって、モデルの特徴量の重要度を解釈する手法です。具体的には、ランダムノイズから始めて、最適化によって、入力データを再構築することで、どの特徴量が予測に貢献しているかを明らかにします。これにより、モデルの予測結果に影響を与える重要な特徴量を特定することができます。|
|Clustering-based Explanation (CBE)|データセット内の類似するサンプルをグループ化して、モデルの予測がグループ全体にどのように影響するかを説明する手法です。CBEは、局所的なサンプルの説明を提供する方法として、クラスタリングアプローチを使用するため、個々のサンプルの説明を生成する代わりに、データセット全体に対する説明を生成することができます。|
|Function Importance Analysis (FIA)|機械学習モデル内の個々の関数の重要性を推定する方法論で、シンプルな線形モデルから複雑な深層学習モデルまで、任意のモデルに適用できる。この手法では、個々の関数をランダムに置換したときの予測の変化に基づいて、関数の重要性を推定する。FIAは、モデル内の異なる関数の相対的な重要性を特定するために使用される。|
|Deep Resemblance Factor Analysis (DeepRFA)|画像分類モデルの解釈性を向上させるために開発された手法で、主成分分析に基づいて各層の特徴マップを抽出し、重要な部分の位置と重要度を可視化します。この手法は、画像内のどの部分が分類に寄与しているかを直感的に理解することができ、モデルの信頼性や改善点を評価することができます。|
|Automated Test-time Augmentation for Model Interpretation (AutoTAI)|モデル解釈における汎用性と安定性を向上するために、テスト時のデータ拡張を自動的に適用する手法です。具体的には、AutoTAIは、入力データにランダムな変換を加えることで、モデルの解釈のロバスト性を高めます。この手法により、モデルの解釈を信頼性の高いものにすることができます。|
|Gradient-Based Explanation for Hierarchical Predictions (Grad-HAP)|ヒエラルキカルなモデルに対して、グラデーションを利用して局所的な予測結果の影響度を測定する手法であり、局所的な重要度スコアを推定するために複数のヘッドを持つモデルを解釈可能なモデルに変換することができます。|
|TreeSHAP|木ベースのモデル（決定木、ランダムフォレスト、勾配ブースティング木など）のSHAP値を効率的に計算するための手法であり、トレーニング時に木構造を利用することで高速化されたSHAP値の近似値を算出します。|
|Fine-grained Interpretation (FiG)|モデルの予測に寄与するピクセルの細かい可視化を可能にする手法であり、物体検出やセマンティックセグメンテーションなど、画像分類以外のタスクにも適用できます。FiGは、グラデーションやGuidedバックプロパゲーションを拡張し、逆伝播中に局所特徴量を保存し、目的関数に合わせて正則化することで、より鮮明な可視化を行います。|
|Counterfactual Prototypes (CF-Proto)|対象となるクラスに最も近いクラスに属するプロトタイプとの違いを示すために、カウンターファクチャルの概念を使用する手法であり、モデルの決定論的および統計的な予測解釈を提供することができます。|
|Gradient-based Explanations for Reinforcement Learning (G-REX)|強化学習におけるQ値関数の出力を説明するために、Gradient REverse eXplanationという手法を用いたモデル解釈手法です。この手法は、各状態におけるQ値関数の出力を最も影響を与えた入力状態を特定するために、状態空間内での勾配の逆方向に探索します。|
|Layer-wise Relevance Propagation-Shapley Sampling (LRP-SS)|ニューラルネットワークの入力特徴量の重要度を決定するための手法であり、レイヤーワイズ・リレバンス・プロパゲーション（LRP）とシャプリー値サンプリングを組み合わせて使用します。LRPは、モデルの出力に対する各入力の寄与を再帰的に計算することで、各入力の重要度を決定します。一方、シャプリー値サンプリングは、入力の部分集合に対する予測値の変化量を計算し、各特徴量の重要度を導出します。LRP-SSは、これら2つの手法を組み合わせることで、ニューラルネットワークの入力特徴量の重要度を効率的に推定します。|








